# cf_http_double_check.py
import requests
import concurrent.futures
import os
import cloudscraper
import threading

# Danh s√°ch API HTTP proxy (VIP + Free)
APIS_HTTP = [
    # Proxyscrape
    "https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=10000&country=all",
    # Proxy-list.download
    "https://www.proxy-list.download/api/v1/get?type=http",
    # OpenProxy
    "https://openproxy.space/list/http",
    # Proxy-list (raw)
    "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
    "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
    "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/http.txt",
    "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/proxies-http.txt",
    # Geonode (API JSON, nh∆∞ng ch·ªâ l·∫•y type HTTP)
    "https://proxylist.geonode.com/api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc&protocols=http",
]

OUT_FILE = "http.txt"
lock = threading.Lock()

# B1: Check proxy s·ªëng (httpbin.org)
def check_alive(proxy):
    test_url = "https://httpbin.org/ip"
    proxies = {"http": f"http://{proxy}", "https": f"http://{proxy}"}
    try:
        r = requests.get(test_url, proxies=proxies, timeout=3)
        if r.status_code == 200:
            return proxy
    except:
        return None
    return None

# B2: Check qua Cloudflare (cdn-cgi/trace) + l∆∞u ngay
def check_cloudflare(proxy):
    test_url = "https://www.cloudflare.com/cdn-cgi/trace"
    proxies = {"http": f"http://{proxy}", "https": f"http://{proxy}"}
    try:
        scraper = cloudscraper.create_scraper()
        r = scraper.get(test_url, proxies=proxies, timeout=5)
        if r.status_code == 200:
            with lock:
                with open(OUT_FILE, "a") as f:
                    f.write(proxy + "\n")
                print(f"‚úÖ L∆∞u proxy: {proxy}")
            return proxy
    except:
        return None
    return None

# L·∫•y proxy t·ª´ API
def fetch_api(url):
    proxies = []
    try:
        if "geonode.com" in url:
            r = requests.get(url, timeout=10).json()
            for p in r.get("data", []):
                ip, port = p.get("ip"), p.get("port")
                if ip and port:
                    proxies.append(f"{ip}:{port}")
        else:
            r = requests.get(url, timeout=10)
            for line in r.text.splitlines():
                if ":" in line:
                    proxies.append(line.strip())
    except:
        pass
    return proxies

def main():
    # Xo√° file c≈©
    if os.path.exists(OUT_FILE):
        os.remove(OUT_FILE)
        print(f"üóëÔ∏è ƒê√£ xo√° file c≈©: {OUT_FILE}")

    # t·∫£i proxy
    all_proxies = []
    print("üåê ƒêang ƒë√†o HTTP proxy t·ª´ API VIP...")
    for url in APIS_HTTP:
        all_proxies.extend(fetch_api(url))

    all_proxies = list(set(all_proxies))
    print(f"üîç T·ªïng proxy HTTP l·∫•y ƒë∆∞·ª£c: {len(all_proxies)}")

    # B1: check proxy s·ªëng
    print("‚ö° ƒêang check proxy s·ªëng...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:
        alive_results = list(executor.map(check_alive, all_proxies))
    alive = [p for p in alive_results if p]
    print(f"‚úÖ Proxy s·ªëng: {len(alive)}/{len(all_proxies)}")

    # B2: check Cloudflare + l∆∞u ngay
    print("üåê ƒêang check proxy qua Cloudflare...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:
        list(executor.map(check_cloudflare, alive))

    # b√°o c√°o cu·ªëi
    with open(OUT_FILE, "r") as f:
        saved = f.read().splitlines()
    print(f"\nüéØ Ho√†n t·∫•t: {len(saved)} proxy v∆∞·ª£t Cloudflare ƒë√£ l∆∞u v√†o {OUT_FILE}")

if __name__ == "__main__":
    main()
